{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dim_date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, lit, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/25 16:42:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DimDateTable\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with a range of dates\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2050-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = spark.sql(f\"\"\"\n",
    "    SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_range\n",
    "\"\"\").selectExpr(\"explode(date_range) as date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+-----+---+-------+---------+\n",
      "|      date| date_id|year|month|day|quarter|  weekday|\n",
      "+----------+--------+----+-----+---+-------+---------+\n",
      "|2000-01-01|20000101|2000|    1| 01|      1| Saturday|\n",
      "|2000-01-02|20000102|2000|    1| 02|      1|   Sunday|\n",
      "|2000-01-03|20000103|2000|    1| 03|      1|   Monday|\n",
      "|2000-01-04|20000104|2000|    1| 04|      1|  Tuesday|\n",
      "|2000-01-05|20000105|2000|    1| 05|      1|Wednesday|\n",
      "|2000-01-06|20000106|2000|    1| 06|      1| Thursday|\n",
      "|2000-01-07|20000107|2000|    1| 07|      1|   Friday|\n",
      "|2000-01-08|20000108|2000|    1| 08|      1| Saturday|\n",
      "|2000-01-09|20000109|2000|    1| 09|      1|   Sunday|\n",
      "|2000-01-10|20000110|2000|    1| 10|      1|   Monday|\n",
      "|2000-01-11|20000111|2000|    1| 11|      1|  Tuesday|\n",
      "|2000-01-12|20000112|2000|    1| 12|      1|Wednesday|\n",
      "|2000-01-13|20000113|2000|    1| 13|      1| Thursday|\n",
      "|2000-01-14|20000114|2000|    1| 14|      1|   Friday|\n",
      "|2000-01-15|20000115|2000|    1| 15|      1| Saturday|\n",
      "|2000-01-16|20000116|2000|    1| 16|      1|   Sunday|\n",
      "|2000-01-17|20000117|2000|    1| 17|      1|   Monday|\n",
      "|2000-01-18|20000118|2000|    1| 18|      1|  Tuesday|\n",
      "|2000-01-19|20000119|2000|    1| 19|      1|Wednesday|\n",
      "|2000-01-20|20000120|2000|    1| 20|      1| Thursday|\n",
      "+----------+--------+----+-----+---+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, floor\n",
    "\n",
    "# Extract date attributes with proper casting and use floor for integer division\n",
    "dim_date_df = date_range \\\n",
    "    .withColumn('date_id', date_format(col('date'), 'yyyyMMdd')) \\\n",
    "    .withColumn('year', date_format(col('date'), 'yyyy')) \\\n",
    "    .withColumn('month', date_format(col('date'), 'MM').cast('int')) \\\n",
    "    .withColumn('day', date_format(col('date'), 'dd')) \\\n",
    "    .withColumn('quarter', (floor((col('month') - 1) / 3) + 1)) \\\n",
    "    .withColumn('weekday', date_format(col('date'), 'EEEE'))\n",
    "\n",
    "# Show the DataFrame\n",
    "dim_date_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dim_date data in CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "dim_date_df.toPandas().to_csv(\"../../data/dim_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysparklesson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
