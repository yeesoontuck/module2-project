{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-bigquery\n",
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99441 rows into project-brazilian-ecommerce.ecommerce_data.customers\n",
      "Loaded 1000163 rows into project-brazilian-ecommerce.ecommerce_data.geolocation\n",
      "Loaded 112650 rows into project-brazilian-ecommerce.ecommerce_data.order_items\n",
      "Loaded 103886 rows into project-brazilian-ecommerce.ecommerce_data.order_payments\n",
      "Loaded 99224 rows into project-brazilian-ecommerce.ecommerce_data.order_reviews\n",
      "Loaded 99441 rows into project-brazilian-ecommerce.ecommerce_data.orders\n",
      "Loaded 32951 rows into project-brazilian-ecommerce.ecommerce_data.products\n",
      "Loaded 3095 rows into project-brazilian-ecommerce.ecommerce_data.sellers\n",
      "Loaded 71 rows into project-brazilian-ecommerce.ecommerce_data.product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "def get_bq_client():\n",
    "    \"\"\"Initialize and return the BigQuery client.\"\"\"\n",
    "    return bigquery.Client()\n",
    "\n",
    "# Reading a CSV file \n",
    "def read_and_preprocess_csv(file_path):\n",
    "    \"\"\"Read CSV and preprocess the data by handling missing values and applying conversions.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Replace NaN values with None (NULL)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    # Return the dataframe after preprocessing\n",
    "    return df\n",
    "\n",
    "# Convert columns to datetime \n",
    "def convert_columns_to_datetime(df, datetime_columns):\n",
    "    \"\"\"Convert specified columns to datetime type.\"\"\"\n",
    "    for column in datetime_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply schema conversions\n",
    "def apply_schema_conversions(df, schema_dict):\n",
    "    \"\"\"Apply the specified schema conversions to the DataFrame.\"\"\"\n",
    "    schema_dict_filtered = {col: dtype for col, dtype in schema_dict.items() if col in df.columns}\n",
    "        # Convert integer columns to nullable Int64 type\n",
    "    for col, dtype in schema_dict_filtered.items():\n",
    "        if dtype == int:  # Check if the column is supposed to be an integer\n",
    "            df[col] = df[col].astype('Int64')  # Use pandas' nullable Int64 type\n",
    "        else:\n",
    "            df[col] = df[col].astype(dtype)  # Regular casting for other types\n",
    "\n",
    "    #df = df.astype(schema_dict_filtered)\n",
    "    return df\n",
    "\n",
    "# Load the DataFrame into BigQuery\n",
    "def load_data_to_bq(df, project_id, dataset_id, table_name):\n",
    "    \"\"\"Load the data to a BigQuery table.\"\"\"\n",
    "    destination_table = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "    bq_client = get_bq_client()\n",
    "\n",
    "    # Load the DataFrame into BigQuery table\n",
    "    job = bq_client.load_table_from_dataframe(df, destination_table)\n",
    "    job.result()  # Wait for the job to complete\n",
    "\n",
    "    print(f\"Loaded {job.output_rows} rows into {destination_table}\")\n",
    "\n",
    "# Map CSV files to their BigQuery table names\n",
    "def map_file_to_table(csv_files):\n",
    "    \"\"\"Map file paths to BigQuery table names.\"\"\"\n",
    "    table_name_mapping = {\n",
    "        '../data/olist_customers_dataset.csv': 'customers',\n",
    "        '../data/olist_geolocation_dataset.csv': 'geolocation',\n",
    "        '../data/olist_order_items_dataset.csv': 'order_items',\n",
    "        '../data/olist_order_payments_dataset.csv': 'order_payments',\n",
    "        '../data/olist_order_reviews_dataset.csv': 'order_reviews',\n",
    "        '../data/olist_orders_dataset.csv': 'orders',\n",
    "        '../data/olist_products_dataset.csv': 'products',\n",
    "        '../data/olist_sellers_dataset.csv': 'sellers',\n",
    "        '../data/product_category_name_translation.csv': 'product_category_name_translation',\n",
    "    }\n",
    "    return {file_path: table_name_mapping[file_path] for file_path in csv_files}\n",
    "\n",
    "# Process each CSV file\n",
    "def process_csv_files(csv_files, schema_dict, project_id, dataset_id):\n",
    "    \"\"\"Loop through each CSV file and process it.\"\"\"\n",
    "    for file_path in csv_files:\n",
    "        # Read the CSV into a pandas DataFrame\n",
    "        df = read_and_preprocess_csv(file_path)\n",
    "\n",
    "        # Convert datetime columns\n",
    "        datetime_columns = [\n",
    "            'shipping_limit_date', 'review_creation_date', 'review_answer_timestamp',\n",
    "            'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "            'order_delivered_customer_date', 'order_estimated_delivery_date'\n",
    "        ]\n",
    "        df = convert_columns_to_datetime(df, datetime_columns)\n",
    "\n",
    "        # Apply schema conversions \n",
    "        df = apply_schema_conversions(df, schema_dict)\n",
    "\n",
    "        # Get table name \n",
    "        table_name = map_file_to_table(csv_files).get(file_path)\n",
    "\n",
    "        # Load data into BigQuery\n",
    "        load_data_to_bq(df, project_id, dataset_id, table_name)\n",
    "\n",
    "# Define BigQuery dataset and table name\n",
    "project_id = 'project-brazilian-ecommerce'\n",
    "dataset_id = 'ecommerce_data'\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema_dict = {\n",
    "    'customer_id': str,\n",
    "    'customer_unique_id': str,\n",
    "    'customer_zip_code_prefix': str,\n",
    "    'customer_city': str,\n",
    "    'customer_state': str,\n",
    "    'geolocation_zip_code_prefix': str,\n",
    "    'geolocation_lat': float,\n",
    "    'geolocation_lng': float,\n",
    "    'geolocation_city': str, \n",
    "    'geolocation_state': str,\n",
    "    'order_id': str,\n",
    "    'order_item_id': int,\n",
    "    'product_id': str,\n",
    "    'seller_id': str,\n",
    "    'shipping_limit_date': 'datetime64[ns]',\n",
    "    'price': float,\n",
    "    'freight_value': float,\n",
    "    'payment_sequential': int, \n",
    "    'payment_type': str,\n",
    "    'payment_installments': int,\n",
    "    'payment_value': float,    \n",
    "    'review_id': str,\n",
    "    'review_score': int,\n",
    "    'review_comment_title': str,\n",
    "    'review_comment_message': str,\n",
    "    'review_creation_date': 'datetime64[ns]',\n",
    "    'review_answer_timestamp': 'datetime64[ns]',\n",
    "    'order_status': str,\n",
    "    'order_purchase_timestamp': 'datetime64[ns]',\n",
    "    'order_approved_at': 'datetime64[ns]',\n",
    "    'order_delivered_carrier_date': 'datetime64[ns]',\n",
    "    'order_delivered_customer_date': 'datetime64[ns]',\n",
    "    'order_estimated_delivery_date': 'datetime64[ns]',\n",
    "    'product_category_name': str,\n",
    "    'product_name_lenght': int,\n",
    "    'product_description_lenght': int,\n",
    "    'product_photos_qty': int,\n",
    "    'product_weight_g': int,\n",
    "    'product_length_cm': int,\n",
    "    'product_height_cm': int,\n",
    "    'product_width_cm': int,\n",
    "    'seller_zip_code_prefix': str,\n",
    "    'seller_city': str,\n",
    "    'seller_state': str,\n",
    "    'product_category_name_english': str\n",
    "}\n",
    "\n",
    "# List of CSV files to process\n",
    "csv_files = [\n",
    "    '../data/olist_customers_dataset.csv',\n",
    "    '../data/olist_geolocation_dataset.csv',\n",
    "    '../data/olist_order_items_dataset.csv',\n",
    "    '../data/olist_order_payments_dataset.csv',\n",
    "    '../data/olist_order_reviews_dataset.csv',\n",
    "    '../data/olist_orders_dataset.csv',\n",
    "    '../data/olist_products_dataset.csv',\n",
    "    '../data/olist_sellers_dataset.csv',\n",
    "    '../data/product_category_name_translation.csv',\n",
    "]\n",
    "\n",
    "# Process all CSV files\n",
    "process_csv_files(csv_files, schema_dict, project_id, dataset_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elt1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
